{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_generation2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Yk2dacpGuiop"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9HUoN78pyuZ"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import string\n",
        "import textwrap\n",
        "import warnings\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "import keras.utils as ku\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.colab import output\n",
        "from IPython.display import Javascript\n",
        "from keras.callbacks import LambdaCallback, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers import Input, LSTM, Dense, Dropout, Embedding, Attention\n",
        "from keras.layers import concatenate, Reshape, SpatialDropout1D\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from numpy.random import seed\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow import config as config\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOHvtlrYumnk",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "input_len = max_sequence_len - 1\n",
        "_model = Sequential(name=\"Text_generation\")\n",
        "\n",
        "# Add Input Embedding Layer\n",
        "_model.add(Embedding(total_words, 10, input_length=input_len, name=\"Input\"))\n",
        "# Add Hidden Layer 1 - LSTM Layer\n",
        "_model.add(LSTM(256, name=\"Hidden_1\", dropout=0.5, return_sequences=True))\n",
        "#model.add(Dropout(0.1, name=\"Dropout_1\"))\n",
        "# Add Hidden Layer 2 - LSTM Layer\n",
        "_model.add(LSTM(128, name=\"Hidden_2\", dropout=0.5, return_sequences=True))\n",
        "#model.add(Dropout(0.1, name=\"Dropout_2\"))\n",
        "# Add Hidden Layer 3 - LSTM Layer\n",
        "_model.add(LSTM(64, name=\"Hidden_3\", dropout=0.5, return_sequences=True))\n",
        "#model.add(Dropout(0.1, name=\"Dropout_3\"))\n",
        "# Add Hidden Layer 4 - LSTM Layer\n",
        "_model.add(LSTM(32, name=\"Hidden_4\", dropout=0.5, return_sequences=False))\n",
        "#model.add(Dropout(0.1, name=\"Dropout_4\"))\n",
        "# Add Output Layer\n",
        "_model.add(Dense(total_words, activation='softmax', name=\"Output\"))\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "adagrad = tf.keras.optimizers.Adagrad(learning_rate=0.001)\n",
        "rmsprop = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "sgd = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "\n",
        "optimizer = adagrad\n",
        "\n",
        "_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hWz5C9Aq-N0"
      },
      "source": [
        "#Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lfge2-Hp5oY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e683f5f-1828-45af-8f78-72a6a0223501"
      },
      "source": [
        "path = tf.keras.utils.get_file(\n",
        "    \"sherlock\",\n",
        "    \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        ")\n",
        "\n",
        "path = \"main.txt\"\n",
        "\n",
        "with open(path) as f:\n",
        "    text = f.read().splitlines()\n",
        "\n",
        "arr = []\n",
        "for g in range(32):\n",
        "    l = text\n",
        "    random.shuffle(l)\n",
        "    arr += l\n",
        "\n",
        "text += arr\n",
        "print(len(text))\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    return txt \n",
        "\n",
        "corpus = [clean_text(x) for x in text]"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkB438rNrCzO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f6f181b-fc38-4800-bc35-4af647e963f3"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "def get_sequence_of_tokens(corpus):\n",
        "    ## tokenization\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "    \n",
        "    ## convert data to sequence of tokens \n",
        "    input_sequences = []\n",
        "    for line in corpus:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences, total_words\n",
        "\n",
        "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
        "total_words"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "756"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDzOQV3etDmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "641d23a7-0470-4aa7-bd58-2cdc2886b9c4"
      },
      "source": [
        "def generate_padded_sequences(input_sequences):\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "    \n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "    label = ku.to_categorical(label, num_classes=total_words)\n",
        "    return predictors, label, max_sequence_len\n",
        "\n",
        "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)\n",
        "print(len(predictors))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "67716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk2dacpGuiop"
      },
      "source": [
        "#Model creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7Qu772TqXqo",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers\n",
        "\n",
        "class AttentionWeightedAverage(Layer):\n",
        "    \"\"\"\n",
        "    Computes a weighted average attention mechanism from:\n",
        "        Zhou, Peng, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li, Hongwei Hao and Bo Xu.\n",
        "        “Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification.”\n",
        "        ACL (2016). http://www.aclweb.org/anthology/P16-2034\n",
        "    How to use:\n",
        "    see: [BLOGPOST]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, return_attention=False, **kwargs):\n",
        "        self.init = initializers.get('uniform')\n",
        "        self.supports_masking = True\n",
        "        self.return_attention = return_attention\n",
        "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'supports_masking': self.supports_masking,\n",
        "            'return_attention': self.return_attention\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(ndim=3)]\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.w = self.add_weight(shape=(input_shape[2], 1),\n",
        "                                 name='{}_w'.format(self.name),\n",
        "                                 initializer=self.init)\n",
        "        self._trainable_weights = [self.w]\n",
        "        super(AttentionWeightedAverage, self).build(input_shape)\n",
        "\n",
        "    def call(self, h, mask=None):\n",
        "        h_shape = K.shape(h)\n",
        "        d_w, T = h_shape[0], h_shape[1]\n",
        "        \n",
        "        logits = K.dot(h, self.w)  # w^T h\n",
        "        logits = K.reshape(logits, (d_w, T))\n",
        "        alpha = K.exp(logits - K.max(logits, axis=-1, keepdims=True))  # exp\n",
        "        \n",
        "        # masked timesteps have zero weight\n",
        "        if mask is not None:\n",
        "            mask = K.cast(mask, K.floatx())\n",
        "            alpha = alpha * mask\n",
        "        alpha = alpha / K.sum(alpha, axis=1, keepdims=True) # softmax\n",
        "        r = K.sum(h * K.expand_dims(alpha), axis=1)  # r = h*alpha^T\n",
        "        h_star = K.tanh(r)  # h^* = tanh(r)\n",
        "        if self.return_attention:\n",
        "            return [h_star, alpha]\n",
        "        return h_star\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return self.compute_output_shape(input_shape)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_len = input_shape[2]\n",
        "        if self.return_attention:\n",
        "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
        "        return (input_shape[0], output_len)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        if isinstance(input_mask, list):\n",
        "            return [None] * len(input_mask)\n",
        "        else:\n",
        "            return None"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgUez8ZbvwYg",
        "outputId": "3dcd8db0-a03a-4beb-a3b2-71a4303f224e"
      },
      "source": [
        "max_length     = max_sequence_len - 1\n",
        "dim_embeddings = 100\n",
        "rnn_size       = 256\n",
        "rnn_amount     = 5\n",
        "dropout        = 0.2\n",
        "num_classes    = total_words\n",
        "\n",
        "inp        = Input(shape=(max_length,), name=\"Input\")\n",
        "embed     = Embedding(num_classes, dim_embeddings, input_length=max_length, name=\"Embed\")(inp)\n",
        "embed     = SpatialDropout1D(dropout, name=\"Dropout\")(embed)\n",
        "\n",
        "rnn_list = []\n",
        "for i in range(rnn_amount):\n",
        "    prev_layer = embed if i == 0 else rnn_list[-1]\n",
        "    rnn_list.append(LSTM(rnn_size, return_sequences=True, name=f\"Rnn_{i}\")(prev_layer))\n",
        "\n",
        "rnn_concat = concatenate([embed] + rnn_list, name=\"Concat\")\n",
        "attention  = AttentionWeightedAverage(name=\"AttentionWeighted\")(rnn_concat)\n",
        "output     = Dense(num_classes, name='Output', activation='softmax')(attention)\n",
        "\n",
        "model      = Model(inputs=[inp], outputs=[output])\n",
        "\n",
        "optimizer = Adam(lr = 0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input (InputLayer)              [(None, 44)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embed (Embedding)               (None, 44, 100)      75600       Input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Dropout (SpatialDropout1D)      (None, 44, 100)      0           Embed[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Rnn_0 (LSTM)                    (None, 44, 256)      365568      Dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Rnn_1 (LSTM)                    (None, 44, 256)      525312      Rnn_0[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Rnn_2 (LSTM)                    (None, 44, 256)      525312      Rnn_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Rnn_3 (LSTM)                    (None, 44, 256)      525312      Rnn_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Rnn_4 (LSTM)                    (None, 44, 256)      525312      Rnn_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Concat (Concatenate)            (None, 44, 1380)     0           Dropout[0][0]                    \n",
            "                                                                 Rnn_0[0][0]                      \n",
            "                                                                 Rnn_1[0][0]                      \n",
            "                                                                 Rnn_2[0][0]                      \n",
            "                                                                 Rnn_3[0][0]                      \n",
            "                                                                 Rnn_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "AttentionWeighted (AttentionWei (None, 1380)         1380        Concat[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Output (Dense)                  (None, 756)          1044036     AttentionWeighted[0][0]          \n",
            "==================================================================================================\n",
            "Total params: 3,587,832\n",
            "Trainable params: 3,587,832\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUShfqLvtYM9"
      },
      "source": [
        "#Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRCcN9Mitag7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "7b15ed58-6401-450a-c7e9-4d9ff08c81bd"
      },
      "source": [
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 80})'''))\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"weights/weights.h5\", monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "lamb = LambdaCallback(on_epoch_end=lambda a,b: gc.collect())\n",
        "\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 256\n",
        "h = model.fit(predictors, label, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[checkpoint, lamb])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 80})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "265/265 [==============================] - 26s 77ms/step - loss: 5.7758\n",
            "\n",
            "Epoch 00001: loss improved from inf to 5.39794, saving model to weights/weights.h5\n",
            "Epoch 2/15\n",
            "265/265 [==============================] - 21s 79ms/step - loss: 4.0489\n",
            "\n",
            "Epoch 00002: loss improved from 5.39794 to 3.53676, saving model to weights/weights.h5\n",
            "Epoch 3/15\n",
            "265/265 [==============================] - 21s 79ms/step - loss: 2.0437\n",
            "\n",
            "Epoch 00003: loss improved from 3.53676 to 1.66661, saving model to weights/weights.h5\n",
            "Epoch 4/15\n",
            "265/265 [==============================] - 20s 77ms/step - loss: 0.8109\n",
            "\n",
            "Epoch 00004: loss improved from 1.66661 to 0.67687, saving model to weights/weights.h5\n",
            "Epoch 5/15\n",
            "265/265 [==============================] - 21s 78ms/step - loss: 0.3905\n",
            "\n",
            "Epoch 00005: loss improved from 0.67687 to 0.34063, saving model to weights/weights.h5\n",
            "Epoch 6/15\n",
            "265/265 [==============================] - 21s 78ms/step - loss: 0.2313\n",
            "\n",
            "Epoch 00006: loss improved from 0.34063 to 0.21149, saving model to weights/weights.h5\n",
            "Epoch 7/15\n",
            "265/265 [==============================] - 21s 78ms/step - loss: 0.1624\n",
            "\n",
            "Epoch 00007: loss improved from 0.21149 to 0.15651, saving model to weights/weights.h5\n",
            "Epoch 8/15\n",
            " 87/265 [========>.....................] - ETA: 13s - loss: 0.1414"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7YekcjqxC_h"
      },
      "source": [
        "model.load_weights(\"weights/weights.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti9RDQWNyNcv"
      },
      "source": [
        "model.load_weights(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6RmhVgW_6Uz"
      },
      "source": [
        "model.save(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCKJRmzt4iB3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "11d49e96-19ef-4065-d4ce-baf3ad36fd1a"
      },
      "source": [
        "%matplotlib inline\n",
        "plt.plot(h.history['loss'])\n",
        "plt.title('Model improvment')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV9Z3/8dcn+0IWCGHJRlAsq5BoXLFWdOq41GrVukyrVeswTvuzy8y0te1M25nOdOzULtppte5dXEuldalWa0XHuqIsIqACAkkIEpZsELJ+fn/cA15iVsjNSXLfz8fjPnLPej8Xkvu+5/s953vM3RERkfiVEHYBIiISLgWBiEicUxCIiMQ5BYGISJxTEIiIxDkFgYhInFMQyIhnZqVm5maW1I91rzCz53tYVmJmTWaWOPhVigxfCgIZUma20cxazWx8l/nLgg/z0nAqA3ff7O5j3L0jrBqGGzP7jpn9Juw6JLYUBBKGd4FL902Y2ZFARnjlDL3+HL2IDBUFgYTh18DlUdOfAX4VvYKZ5ZjZr8ys1sw2mdm/mllCsCzRzG4ws+1mtgE4u5tt7zCzGjOrNrP/7E9zT9cmJjNbEmz7QtBk9IiZ5ZnZPWbWYGavRh/BBNt+wcw2BLX9IKrmK8zsr2b2YzPbAXynp/doZqlmVmdmc6L2nW9mzWY2wcxOMbMqM/uqmW0L3ud5ZnaWmb1tZjvN7BtR2yaY2XVmtt7MdpjZg2Y2rst7/oyZbQ7q/maw7AzgG8DFwftf0Y//WxmBFAQShpeAbDObGXxAXwJ0bX74KZADHAZ8hEhwXBks+3vgY0A5UAFc2GXbu4F2YFqwzunA1QdZ6yXAZUAhcDjwInAXMA5YA3y7y/qfCGo6CjgXuCpq2XHABmAi8F89vUd3bwEeIuqoCbgIeNbdtwXTk4C0oK5vAbcBnwaOBj4M/JuZTQ3WvRY4L3iNAmAX8LMudZ8ETAdOA75lZjPd/Qnge8ADQZPZvL7/uWREcnc99BiyB7AR+BvgX4H/Bs4AngKSAAdKgUSgFZgVtd0/AEuC538BroladnqwbRKRD9kWID1q+aXAM8HzK4Dne6itdN9+guklwDejlv8QeDxq+hxgedS0A2dETX8OeDrqdTdHLevrPf4NsD5q2V+By4PnpwDNQGIwnRW89nFR678GnBc8XwOcFrVsMtAW/Hvte89FUctfAS4Jnn8H+E3Yvzd6xPahdkoJy6+B54CpdGkWAsYDycCmqHmbiHz7hci32souy/aZEmxbY2b75iV0WX8g3ot63tzN9Jgu63etq6CHZX29x2eADDM7LnjNMmBx1Lo7/P1O7eYeat1X2xRgsZl1Ri3vIBKa+2yNer6nm/clo5iahiQU7r6JSKfxWUSaQaJtJ/KNdUrUvBKgOnheAxR3WbZPJZEjgvHunhs8st199mDW34uudW2Jmo4e6rfX9xh8yD9I5GjmUuBRd288yJoqgTOj/j1y3T3N3av73PLAmmWUUhBImD4LnOruu6NnRn0I/peZZZnZFOCfeL8f4UHgC2ZWZGZjgeuitq0BngR+aGbZQUfp4Wb2kaF4Q8BXzGysmRUDXwQe6G6lfrxHgHuBi4FPBc8P1i3B60yB/R3P5/Zz2/eA0n2d3jI66T9XQuPu6919aQ+LrwV2E+lcfZ7IB+GdwbLbgD8BK4DX+eARxeVACrCaSMfoIiLt4kPhD0Ta55cDjwF39LJub+8Rd385WF4APH4INd0IPAw8aWaNRDrrj+vntr8Nfu4ws9cPoQYZxsxdR34ig8HMHDjC3deFXYvIQOiIQEQkzikIRETinJqGRETinI4IRETiXEwvKDOzXOB2YA6R85GvcvcXu1nvGCKX7l/i7ot62+f48eO9tLQ0BtWKiIxer7322nZ3z+9uWayvLL4ReMLdLzSzFLoZYTIYa+b7RM797lNpaSlLl/Z0xqGIiHTHzDb1tCxmTUNmlgOcTHAetbu3untdN6teC/wO2NbNMhERibFY9hFMBWqBu4KbjtxuZpnRK5hZIZHRGm+OYR0iItKLWAZBEpGheG9293IiV0he12WdnwBfc/fOrhtHM7OFZrbUzJbW1tbGploRkTgVyz6CKqAquEweIpf5dw2CCuD+YJTI8cBZZtbu7r+PXsndbwVuBaioqPjA+a5tbW1UVVWxd+/eQX4Lw09aWhpFRUUkJyeHXYqIjBIxCwJ332pmlWY23d3fInLDi9Vd1tl34wzM7G4iIyz+ngGqqqoiKyuL0tJSooYeHnXcnR07dlBVVcXUqVP73kBEpB9ifdbQtcA9wRlDG4ArzewaAHe/ZbBeZO/evaM+BADMjLy8PNQ8JiKDKaZB4O7LiTT/ROs2ANz9ikN5rdEeAvvEy/sUkaETN1cW723roKaumY5ODakhIhItboKgtb2T2qYW9rZ19L3yANXV1fHzn/98wNudddZZ1NV1d2mFiMjQiZsgyEhJBGBPa/ug77unIGhv7/21/vjHP5Kbmzvo9YiIDETc3Lw+KTGBlKQE9rQO/hHBddddx/r16ykrKyM5OZm0tDTGjh3L2rVrefvttznvvPOorKxk7969fPGLX2ThwoXA+8NlNDU1ceaZZ3LSSSfxwgsvUFhYyB/+8AfS09MHvVYRka5GXRD8+yNvsnpLQ7fLWto76ej0/UcH/TWrIJtvn9Pzvc+vv/56Vq1axfLly1myZAlnn302q1at2n+K55133sm4ceNobm7mmGOO4YILLiAvL++Afbzzzjvcd9993HbbbVx00UX87ne/49Of/vSA6hQRORijLgh6k2DQ7o4DsTz35thjjz3gPP+bbrqJxYsXA1BZWck777zzgSCYOnUqZWVlABx99NFs3LgxhhWKiLxv1AVBb9/cd7e0s762iSl5meSkx+7K3MzM94dUWrJkCX/+85958cUXycjI4JRTTun2CujU1NT9zxMTE2lubo5ZfSIi0eKmsxggPTkRMxv0DuOsrCwaGxu7XVZfX8/YsWPJyMhg7dq1vPTSS4P62iIih2rUHRH0JiHBSEse/A7jvLw85s+fz5w5c0hPT2fixIn7l51xxhnccsstzJw5k+nTp3P88ccP6muLiByqEXfP4oqKCu96Y5o1a9Ywc+bMfm1fXdfMrt2tzC7IHrFX6Q7k/YqIAJjZa+7edaQHIM6ahiByPUGnO3vbex35WkQkbsRfECRHTh1tjsGFZSIiI9GoCYL+NnGlJCWQmGAxubBsKIy0pjwRGf5GRRCkpaWxY8eOfn1ImhkZKUkjMgj23Y8gLS0t7FJEZBQZFWcNFRUVUVVV1e9x+hua22jc207bjjQSRliH8b47lImIDJZREQTJyckDumPXM29t4+oHX+W+vz+eEw7P63sDEZFRbFQ0DQ1UWVFkxM/llRoCWkQkLoNgbGYKpXkZLK/cFXYpIiKhi8sgACgrzmXZ5jqdhSMicS+mQWBmuWa2yMzWmtkaMzuhy/JzzWylmS03s6VmdlIs64lWVpzLtsYWauo/OACciEg8iXVn8Y3AE+5+oZmlABldlj8NPOzubmZzgQeBGTGuCYCykrFApJ+gIFc3gBGR+BWzIwIzywFOBu4AcPdWdz+gd9bdm/z9tplMYMjaaWZOziIlMUEdxiIS92LZNDQVqAXuMrNlZna7mWV2XcnMPmFma4HHgKu625GZLQyajpb291qBvqQmJTKrIJvlmxUEIhLfYhkEScBRwM3uXg7sBq7rupK7L3b3GcB5wHe725G73+ruFe5ekZ+fP2gFlpfk8kZ1Pe0dGoBOROJXLIOgCqhy95eD6UVEgqFb7v4ccJiZjY9hTQcoK86lua2Dt97r/qYyIiLxIGZB4O5bgUozmx7MOg1YHb2OmU2z4KYAZnYUkArsiFVNXZUXv99hLCISr2J9HcG1wD1mthIoA75nZteY2TXB8guAVWa2HPgZcLEP4Yn9xePSGZeZon4CEYlrMT191N2XA13viHNL1PLvA9+PZQ29MTPKinN1RCAicS1uryzep6w4l3W1TTTubQu7FBGRUCgIinNxh5VV9WGXIiISirgPgnnFGolUROJb3AdBTnoyh+VnsmyzRiIVkfgU90EA7O8w1kikIhKPFARAeXEu25taqdrVHHYpIiJDTkEAlOnCMhGJYwoCYMbkLFKTNBKpiMQnBQGQnJjAnMIcBYGIxCUFQaC8OJdV1fW0aSRSEYkzCoJAWUkuLe2drK3RSKQiEl8UBIGy/ReW6XoCEYkvCoJAYW4648ekskz9BCISZxQEAY1EKiLxSkEQpbwklw21u6nfo5FIRSR+KAii7O8nqNJRgYjEDwVBlLlFOZihO5aJSFxREETJSktmWv4YnTkkInElpkFgZrlmtsjM1prZGjM7ocvyT5nZSjN7w8xeMLN5saynPzQSqYjEm1gfEdwIPOHuM4B5wJouy98FPuLuRwLfBW6NcT19KivJZdeeNjbv3BN2KSIiQyJmQWBmOcDJwB0A7t7q7gc0vrv7C+6+rx3mJaAoVvX0V5nuWCYicSaWRwRTgVrgLjNbZma3m1lmL+t/Fni8uwVmttDMlprZ0tra2ljUut/0iVmkJyeyTB3GIhInYhkEScBRwM3uXg7sBq7rbkUzW0AkCL7W3XJ3v9XdK9y9Ij8/P1b1ApCUmMCRRRqJVETiRyyDoAqocveXg+lFRILhAGY2F7gdONfdd8Swnn4rL85l9ZYGWto7wi5FRCTmYhYE7r4VqDSz6cGs04DV0euYWQnwEHCZu78dq1oGqqw4l9aOTtZoJFIRiQNJMd7/tcA9ZpYCbACuNLNrANz9FuBbQB7wczMDaHf3ihjX1KeykqDDePOu/Z3HIiKjVUyDwN2XA10/2G+JWn41cHUsazgYk3PSmZidqn4CEYkLurK4BxqJVETihYKgB2XFY9m4Yw87d7eGXYqISEwpCHqwr29ghY4KRGSUUxD0YG5RDgmG7lgmIqOegqAHmalJfGhilvoJRGTUUxD0oqw4lxUaiVRERjkFQS/KinOpb27j3e27wy5FRCRmFAS92H9hmZqHRGQUUxD04ogJWWSmJCoIRGRUUxD0IjHBmFukC8tEZHRTEPShrCSXNTUN7G3TSKQiMjopCPpQVpxLW4fz5paGsEsREYkJBUEfynXrShEZ5RQEfZiQnUZBTpqCQERGLQVBP5SV5LJs866wyxARiQkFQT+UFedStauZ7U0tYZciIjLoFAT9UFY8FoDlm9U8JCKjj4KgH44szCExwdRPICKjUkyDwMxyzWyRma01szVmdkKX5TPM7EUzazGzf4llLYciPSWR6RqJVERGqVgfEdwIPOHuM4B5wJouy3cCXwBuiHEdh6ysJDISaWenRiIVkdElZkFgZjnAycAdAO7e6u4HfKV2923u/irQFqs6BktZcS6NLe1s2N4UdikiIoMqlkcEU4Fa4C4zW2Zmt5tZ5sHsyMwWmtlSM1taW1s7uFX2074Ly5apw1hERplYBkEScBRws7uXA7uB6w5mR+5+q7tXuHtFfn7+YNbYb4fnjyErNUn9BCIy6sQyCKqAKnd/OZheRCQYRqSEBGNesUYiFZHRJ2ZB4O5bgUozmx7MOg1YHavXGwplxbms3dpIc6tGIhWR0SMpxvu/FrjHzFKADcCVZnYNgLvfYmaTgKVANtBpZl8CZrn7sBzqs6w4l45OZ9WWeo4pHRd2OSIigyKmQeDuy4GKLrNviVq+FSiKZQ2Daf+tKzfXKQhEZNTQlcUDMH5MKkVj01lWqQHoRGT0UBAMUFlxrsYcEpFRRUEwQGXFuWyp38u2hr1hlyIiMigUBANUHvQTLNNppCIySigIBmh2QQ5JGolUREYRBcEApSUnMnNytvoJRGTUUBAchLLiXFZW1dGhkUhFZBRQEByEsuJcdrd2sG6bRiIVkZFPQXAQ9nUYL9f1BCIyCigIDsLU8ZnkpCerw1hERoV+BYGZZZpZQvD8Q2b2cTNLjm1pw5dZZCRS3ZtAREaD/h4RPAekmVkh8CRwGXB3rIoaCcqKc3n7vUZ2t7SHXYqIyCHpbxCYu+8Bzgd+7u6fBGbHrqzhr7w4l06HlVX1YZciInJI+h0EZnYC8CngsWBeYmxKGhnmFe/rMFbzkIiMbP0Ngi8BXwcWu/ubZnYY8Ezsyhr+xmWmMCUvQ2cOiciI16/7Ebj7s8CzAEGn8XZ3/0IsCxsJyopzeWnDjrDLEBE5JP09a+heM8s2s0xgFbDazL4S29KGv7LiXN5raKGmvjnsUkREDlp/m4b23T7yPOBxYCqRM4fiWlnx+3csExEZqfobBMnBdQPnAQ+7exsQ9wPtzCrIJiUxQR3GIjKi9TcIfgFsBDKB58xsCtDnDebNLNfMFpnZWjNbE5x5FL3czOwmM1tnZivN7KiBvoEwpSYlMrMgW/cmEJERrV9B4O43uXuhu5/lEZuABf3Y9EbgCXefAcwD1nRZfiZwRPBYCNzc/9KHhxMPz2Ppxp288u7OsEsRETko/e0szjGzH5nZ0uDxQyJHB71uA5wM3AHg7q3u3vWr87nAr4JweQnINbPJA38b4fn8gmkUj8vgyw8sp765LexyREQGrL9NQ3cCjcBFwaMBuKuPbaYCtcBdZrbMzG4PzjqKVghURk1XBfMOYGYL94VQbW1tP0seGmNSk7jxknLea9jLNxa/gXvcd52IyAjT3yA43N2/7e4bgse/A4f1sU0ScBRws7uXA7uB6w6mSHe/1d0r3L0iPz//YHYRU2XFuXz5ox/isZU1LHqtKuxyREQGpL9B0GxmJ+2bMLP5QF8nz1cBVe7+cjC9iEgwRKsGiqOmi4J5I841Hzmc46aO49sPv8nG7bvDLkdEpN/6GwTXAD8zs41mthH4X+AfetvA3bcClWY2PZh1GrC6y2oPA5cHZw8dD9S7e02/qx9GEhOMH19cRnJiAl+8fxltHZ1hlyQi0i/9PWtohbvPA+YCc4OmnlP7sem1wD1mthIoA75nZteY2TXB8j8CG4B1wG3A5wb6BoaTgtx0/vv8I1lRVc+Pn3o77HJERPrFDrZz08w2u3vJINfTp4qKCl+6dOlQv+yAfG3RSh58rZJ7rz6eEw7PC7scERHM7DV3r+hu2aHcqtIOYdtR7VvnzKI0L5MvP7Ccuj2tYZcjItKrQwkCnSfZg8zUJG66pJwdu1v4+kM6pVREhrdeg8DMGs2soZtHI1AwRDWOSEcW5fDPp0/n8VVbeXBpZd8biIiEpNcgcPcsd8/u5pHl7v26l0E8W/jhwzjx8Dy+8/Bq1tc2hV2OiEi3DqVpSPqQkGD86KIyUpMT+NL9y2lt1ymlIjL8KAhibFJOGt+/YC5vVNfzw6feCrscEZEPUBAMgb+dPYm/O66EXzy7gb+u2x52OSIiB1AQDJF/O3sWh+dn8k8PLmfnbp1SKiLDh4JgiKSnJHLjJeXs3N3K1363UqeUisiwoSAYQnMKc/jaGTN4avV73PvK5rDLEREBFARD7qr5U/nwEeP57qOrWbetMexyREQUBEMtIcH44SfnkZGSxLX3LaelvSPskkQkzikIQjAhO40fXDiXNTUN/OAJnVIqIuFSEITktJkTuez4Kdz+/Ls89/bwuv2miMQXBUGIvnn2TI6YMIZ//u0KdjS1hF2OiMQpBUGI0pITuenScuqb2/jqIp1SKiLhUBCEbObkbK47YwZPr93Gr1/aFHY5IhKHFATDwJXzSzllej7/9dga3n5Pp5SKyNCKaRAEN7t/w8yWm9kH7i9pZmPNbLGZrTSzV8xsTizrGa7MjB9cOI+stCS+cN8y9rbplFIRGTpDcUSwwN3LerhX5jeA5e4+F7gcuHEI6hmW8rNS+cGF81i7tZHrH18bdjkiEkfCbhqaBfwFwN3XAqVmNjHcksKzYMYErjixlLtf2Mgzb20LuxwRiROxDgIHnjSz18xsYTfLVwDnA5jZscAUoKjrSma20MyWmtnS2trRfc79dWfOYMakLL7y2xXUNuqUUhGJvVgHwUnufhRwJvB5Mzu5y/LrgVwzWw5cCywDPtBA7u63unuFu1fk5+fHuORwpSVHRilt3NvOeT/7K79+aZP6DEQkpmIaBO5eHfzcBiwGju2yvMHdr3T3MiJ9BPnAhljWNBJMn5TFL686lgnZqfzb71fxkR88wx3Pv0tzqwJBRAZfzILAzDLNLGvfc+B0YFWXdXLNLCWYvBp4zt0bYlXTSHL8YXk89I8ncs/VxzF1fCbffXQ1J33/L9y8ZD1NLe1hlycio0hSDPc9EVhsZvte5153f8LMrgFw91uAmcAvzcyBN4HPxrCeEcfMmD9tPPOnjefVjTv56V/W8f0n1nLLs+u5av5UrjixlJyM5LDLFJERzkbasAYVFRW+dOkHLkmIGysq6/jpX9bx5zXvMSY1ictPmMJnT5pK3pjUsEsTkWHMzF7r4TR+BcFItXpLAz97Zh1/XFVDWlIinzquhIUnH8aE7LSwSxORYUhBMIqt29bIz59Zzx9WbCExwbjkmGL+4SOHU5ibHnZpIjKMKAjiwKYdu7l5yXp+93oV7nDBUUV8bsHhTMnLDLs0ERkGFARxpLqumV88u577X62kvaOTc8sK+fyCw5k2ISvs0kQkRAqCOLStYS+3PreBe17ezN72Ds6aM5nPL5jGrILssEsTkRAoCOLYjqYW7vzru/zyhU00tbRz6owJlOZl0t7ZSVuH09bRSXtHJ22dTntHJ+0d3uV58LOjk/ZgfluH095lflKCMbsgh6Om5FJePJayklzG60wmkWFDQSDU72nj7hc28puXN9Hc2kFSopGUkEByopGUaCQnJER+JiaQlJhAcoK9P51gkXnBNh9YP8FobutgZVU9a2oaaO+M/E6VjMugvCSX8uJcykvGMnNyNilJYY9zKBKfFAQyZJpbO1i1pZ5lm3exbHMdr2/exXsNkcHzUpISOLIwh/LiXI6aMpbyklwm5+jsJpGhoCCQUNXUN/P6prpIOFTW8UZ1Pa3tnQBMyk6LHDWURI4ajizMIS05MeSK+6+j09lQ28SqLfW8u30PFVPGMn/aeBITLOzSRA7QWxDEcogJEQAm56Rz9tx0zp47GYDW9k7W1DTsD4Zlm+t4fNVWAJISjJmTs/eHw5yCHIrHZQyLcGhp7+DtrU28uaWeVVvqeXNLA2tqGtjb1nnAehOzUzm3rJBPlBcyc7I652X40xGBDAvbm1pYtrluf5PSiqo69kSNtjohK5XicRmUjMugeGz6+8/HZTAxO23Qv4E3tbSzpqaBN6vrWbWlgTe3NPDOe437+z/GpCYxqyCb2QXZzCnIYXZhNsVjM3j27Voeer2aJW9to73TmTk5m/PLCzm3rEBXfUuo1DQkI05Hp/PW1kbefq+Ryp172LxzD5W79lC5s5ma+mY6o35tUxITKAzCoXhs+v6AiIRGRp8D8+3c3cqbwTf8N7dEPvzf3bGbfX8aeZkpzC7Mef9DvyCbknEZJPQSPjuaWnh0ZQ0PLatmRWUdCQbzp43ngqOKOH32RDJSdDAuQ0tBIKNKa3snW+qa94fD5p17qNr5/nTdnrYD1s9OSzrgCKJ4bDo7drfu/9DfUr93/7qFuenMLshmdkEOcwojPydmpxKMontQ1tc28ftl1Tz0ejXVdc1kpiTyt3MmcX55ESccnqf+BBkSCgKJKw1726jcuSd4dAmMXc20tndiBlPHZ+7/hj+nMIdZk7MZm5nS9wscpM5O59WNO1m8rJrHVtbQ2NLOpOw0zi0v4PzyIqZP0tXfEjsKApFAZ6dT29TCmNQkMlPDa57Z29bBn9e8x+LXq1nydi0dnc7sgmw+UV7Ix8sKmJCl/gQZXAoCkWFse1MLj6zYwuJl1aysqicxwfjwEeP5RHkhp8+aRHpK+GdMycinIBAZIdZta+Sh16v5/bJqttTvZUxqEmfOmcTFxxRz9JSxh9RXIfFNQSAywnR2Oi+/u5PFy6p4bGUNu1s7OGLCGC45toTzywtj2pcho1NoQWBmG4FGoANo71qEmeUAvwFKiFzcdoO739XbPhUEEm92t7Tz6Mot3PdKJcsr60hJTOCMOZO45NhiTjgsT0cJ0i9hB0GFu2/vYfk3gBx3/5qZ5QNvAZPcvbWnfSoIJJ6tqWnggVcreej1Khr2tlOal8HFx5Rw4dFF5GdptFfpWW9BEPZQkA5kWeQrzRhgJ9Aebkkiw9fMydl85+OzeeWbf8OPL57HhOw0vv/EWk7476e55tevseStbXR0jqzmXglfrI8I3gV2EfnA/4W739pleRbwMDADyAIudvfHutnPQmAhQElJydGbNm2KWc0iI8362iYeeLWSRa9VsXN3K4W56VxUUcxFxxRpdFfZL8ymoUJ3rzazCcBTwLXu/lzU8guB+cA/AYcH68xz94ae9qmmIZHutbZ38tTq97j/1c383zvbSTA4ZfoELjmmmFNnTCApMewGAAlTaKOPunt18HObmS0GjgWei1rlSuB6j6TRuuAIYgbwSizrEhmNUpISOHvuZM6eO5nNO/bwwNLN/HZpFQvXbmNCViqfrCji4ooSSvIywi5VhpmYHRGYWSaQ4O6NwfOngP9w9yei1rkZeM/dv2NmE4HXiRwRdNu5DDoiEBmI9o5Onnmrlvtf2cwzb22j0+GkaeO55NhiPjprIqlJulgtXoTSNGRmhwGLg8kk4F53/y8zuwbA3W8xswLgbmAyYESODn7T234VBCIHp6a+md8ureKBVyuprmumICeN686ayTlzJ+sU1DigC8pEZL+OTue5d2q54U9v8eaWBo4tHce3zpnFnMKcsEuTGBrOp4+KyBBLTDAWTJ/Aw//vJP77/CNZV9vEOf/7PF9/6A12NLWEXZ6EQEEgEqcSE4xLjy3hmX85hStPnMpvl1ay4IYl3Pn8u7R1dPa9Axk1FAQicS4nPZlvnTOLJ770YeYV5/Ifj67mrBv/j/97pzbs0mSIKAhEBIBpE7L41VXHctvlFbR2dHLZHa/w979ayqYdu8MuTWJMQSAi+5kZH501kSe/fDJfPWM6f123nY/+6Dn+54m17G7R6C+jlYJARD4gNSmRz50yjWf+5RQ+Nm8yP1+ynlN/uITFy6oYaWcaSt8UBCLSo4nZafzoojIe+gWgD9cAAAv1SURBVNyJTMpO48sPrOCCm19gRWVd2KXJIFIQiEifjioZy+LPzecHF85l885mzv3ZX/nKb1ewrXFv2KXJIFAQiEi/JCQYn6wo5pl/+Qj/cPJh/H55Nafe8Cy3Pree1nadbjqSKQhEZECy0pL5+lkz+dOXTubYqeP43h/XcsZPnuOZtdvCLk0OkoaYEJFD8sxb2/juI6vZsH03C6bn84mjihg/JoX8MankZ6WSk56ssYyGgdCGoRaR0W/B9AnMP3w8v3pxIzf++R2eeevAC9GSE428zEgojB+TEvyMPPY9z89KIX9MGtnpSQqNECgIROSQpSQlcPWHD+PvjithS10z2xpb2N7UyvbGFmqbWvb/rG1qYXVNAzuaWmnv5paaKYkJ5B0QFpHnk7LTOGdeAbkZKSG8u9FPQSAigyYjJYlpE7KYNiGr1/U6O5365rYDQyIIj8jPFrbW72VVdT07drfS0enc8OTbfOG0I7js+CmkJKl7czApCERkyCUkGGMzUxibmcKHJvYdGmu2NnD942v57qOr+c1Lm/j6mTP46KyJakYaJIpVERnWEhKM2QU5/OqqY7nrimNIMFj469f4u9te5s0t9WGXNyooCERkRDAzFsyYwBNfOpn/OHc2a7c28LGfPs9XF61gW4MubDsUCgIRGVGSExO4/IRSlnxlAVefNJXFy6o55YYl3PT0OzS3doRd3ogU0+sIzGwj0Ah0AO1dz2E1s68Anwomk4CZQL677+xpn7qOQESibdqxm+sfX8vjq7YyOSeNr54xnXPnFZKQoP6DaKHdszgIggp3396Pdc8Bvuzup/a2noJARLrz8oYd/Odja3ijup55RTn868dmcUzpuLDLGjZGyj2LLwXuC7sIERmZjjssjz98fj4/umge7zW08MlbXuRz97zG5h17wi5t2Iv1EcG7wC7AgV+4+609rJcBVAHTemsWAh0RiEjf9rS2c9tz73LLs+vp6HSunF/K50+dRnZactilhSbMpqFCd682swnAU8C17v5cN+tdDHza3c/pYT8LgYUAJSUlR2/atClmNYvI6LG1fi83PPkWv3u9irEZKXz5ox/i0mOKSUocTo0hQyO0IOhSxHeAJne/oZtli4Hfuvu9fe1HRwQiMlCrquv57qOrefndnRwxYQzfPHsmp0yfEHZZQyqUPgIzyzSzrH3PgdOBVd2slwN8BPhDrGoRkfg2pzCH+xcezy8uO5q2jk6uuOtVLr/zFd7a2hh2acNCLIeYmAgsDi4BTwLudfcnzOwaAHe/JVjvE8CT7r47hrWISJwzM/529iQWTJ/Ar17cyE1Pv8Pf/uQ5FkzP54r5U/nwtPFxe8qp7kcgInFp1+5W7n5hI/e8vJntTS0cNj6Tz5xYygVHFzEmdXgNw9bW0cnz72xncm4aMyZlH9Q+hkUfwWBREIjIYGpt7+SPb9Rw9wsbWV5Zx5jUJD5ZUcRnTiildHxmaHV1dDovv7uDR1bU8PiqGur2tHHZ8VP47nlzDmp/CgIRkX5YtnkXv3xhI4+9UUN7p3PKh4a22cjdWVZZxyMrtvDYyhq2NbaQkZLIR2dN5Jy5BZz8ofyDHoJbQSAiMgDbGvdy78ub+c1LQbNRfiafOSE2zUbuzpqaRh5ZuYVHVmyhalczKUkJLJiezznzCjhtxkTSUxIP+XUUBCIiB2Ffs9FdL2xkxSA3G22obeKRFTU8snIL67Y1kZhgnDRtPOfMK+D02RMH/eI3BYGIyCEajGaj6rpmHl2xhUdWbmFVdQNmcGzpOM6ZV8CZcyaRNyY1ZvUrCEREBsm2hr3c8/Lm9882ys/kihNLOf+o7puNahtbeHxVDQ8v38LSTbsAmFecyzlzJ3P23MlMzkkfkroVBCIig6xrs1FWahIXBs1GYzNS+NObW3l4xRZeWL+dTofpE7P4eFkBH5s7mSl5Q382koJARCSGujYbJSUYbR3OlLwMPj6vgI/NLWD6pN7vzRxrvQXB8LpqQkRkBCovGUt5yVi+cdZM7n+1kt2t7Zx95GSOLMwhGF1hWFMQiIgMkgnZaXzhtCPCLmPA4m8sVhEROYCCQEQkzikIRETinIJARCTOKQhEROKcgkBEJM4pCERE4pyCQEQkzo24ISbMrBbYdJCbjwe2D2I5sTaS6h1JtcLIqnck1Qojq96RVCscWr1T3D2/uwUjLggOhZkt7WmsjeFoJNU7kmqFkVXvSKoVRla9I6lWiF29ahoSEYlzCgIRkTgXb0Fwa9gFDNBIqnck1Qojq96RVCuMrHpHUq0Qo3rjqo9AREQ+KN6OCEREpAsFgYhInIubIDCzM8zsLTNbZ2bXhV1PT8ys2MyeMbPVZvammX0x7Jr6w8wSzWyZmT0adi29MbNcM1tkZmvNbI2ZnRB2Tb0xsy8HvwerzOw+M0sLu6ZoZnanmW0zs1VR88aZ2VNm9k7wc2yYNe7TQ60/CH4XVprZYjPLDbPGaN3VG7Xsn83MzWz8YLxWXASBmSUCPwPOBGYBl5rZrHCr6lE78M/uPgs4Hvj8MK412heBNWEX0Q83Ak+4+wxgHsO4ZjMrBL4AVLj7HCARuCTcqj7gbuCMLvOuA5529yOAp4Pp4eBuPljrU8Acd58LvA18faiL6sXdfLBezKwYOB3YPFgvFBdBABwLrHP3De7eCtwPnBtyTd1y9xp3fz143kjkg6ow3Kp6Z2ZFwNnA7WHX0hszywFOBu4AcPdWd68Lt6o+JQHpZpYEZABbQq7nAO7+HLCzy+xzgV8Gz38JnDekRfWgu1rd/Ul3bw8mXwKKhrywHvTwbwvwY+CrwKCd6RMvQVAIVEZNVzHMP1wBzKwUKAdeDreSPv2EyC9mZ9iF9GEqUAvcFTRj3W5mmWEX1RN3rwZuIPLNrwaod/cnw62qXya6e03wfCswMcxiBuAq4PGwi+iNmZ0LVLv7isHcb7wEwYhjZmOA3wFfcveGsOvpiZl9DNjm7q+FXUs/JAFHATe7ezmwm+HTbPEBQdv6uUQCrADINLNPh1vVwHjk/PRhf466mX2TSLPsPWHX0hMzywC+AXxrsPcdL0FQDRRHTRcF84YlM0smEgL3uPtDYdfTh/nAx81sI5Emt1PN7DfhltSjKqDK3fcdYS0iEgzD1d8A77p7rbu3AQ8BJ4ZcU3+8Z2aTAYKf20Kup1dmdgXwMeBTPrwvrDqcyJeCFcHfWxHwuplNOtQdx0sQvAocYWZTzSyFSIfbwyHX1C0zMyJt2Gvc/Udh19MXd/+6uxe5eymRf9e/uPuw/Nbq7luBSjObHsw6DVgdYkl92Qwcb2YZwe/FaQzjzu0oDwOfCZ5/BvhDiLX0yszOINKs+XF33xN2Pb1x9zfcfYK7lwZ/b1XAUcHv9SGJiyAIOoP+H/AnIn9ID7r7m+FW1aP5wGVEvlkvDx5nhV3UKHItcI+ZrQTKgO+FXE+PgiOXRcDrwBtE/l6H1ZAIZnYf8CIw3cyqzOyzwPXAR83sHSJHNdeHWeM+PdT6v0AW8FTwt3ZLqEVG6aHe2LzW8D4SEhGRWIuLIwIREemZgkBEJM4pCERE4pyCQEQkzikIRETinIJApAsz64g6dXf5YI5Wa2al3Y0mKRKmpLALEBmGmt29LOwiRIaKjghE+snMNprZ/5jZG2b2iplNC+aXmtlfgjHtnzazkmD+xGCM+xXBY9/wEIlmdltwn4EnzSw9tDclgoJApDvpXZqGLo5aVu/uRxK5IvUnwbyfAr8MxrS/B7gpmH8T8Ky7zyMyptG+q9mPAH7m7rOBOuCCGL8fkV7pymKRLsysyd3HdDN/I3Cqu28IBgbc6u55ZrYdmOzubcH8Gncfb2a1QJG7t0TtoxR4KrhpC2b2NSDZ3f8z9u9MpHs6IhAZGO/h+UC0RD3vQH11EjIFgcjAXBz188Xg+Qu8fwvJTwH/Fzx/GvhH2H9P55yhKlJkIPRNROSD0s1sedT0E+6+7xTSscHIpS3ApcG8a4nc9ewrRO6AdmUw/4vArcGokR1EQqEGkWFGfQQi/RT0EVS4+/awaxEZTGoaEhGJczoiEBGJczoiEBGJcwoCEZE4pyAQEYlzCgIRkTinIBARiXP/H8o0C8tzpblWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO0wE1SfthxV"
      },
      "source": [
        "#Text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3id-PglRtmLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c092fefa-f2b8-4ee5-dab7-e6db7440b6cf"
      },
      "source": [
        "%matplotlib inline\n",
        "generated, probs = generate_text(\"Fladdman:\", 100, model, max_sequence_len, 0.2)\n",
        "\n",
        "\n",
        "#plt.plot(probs)\n",
        "#plt.legend(['probability'], loc='upper right')\n",
        "#plt.title('Prediction')\n",
        "#plt.ylabel('Chance')\n",
        "#plt.xlabel('Letters')\n",
        "#plt.show()\n",
        "\n",
        "w = textwrap.TextWrapper(width=170, break_long_words=False)\n",
        "new_text = \"\\n\".join(w.wrap(generated)).lstrip()\n",
        "print(new_text)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fladdman: you the the the the the the the the the the the the the the that the the the the the the the the and the heroes the heroes heroes that heroes the that heroes\n",
            "heroes the heroes that that heroes heroes that heroes heroes heroes heroes heroes heroes heroes heroes heroes heroes heroes heroes that heroes heroes heroes heroes that\n",
            "heroes that heroes the the heroes that that that that the that that the heroes heroes the heroes that heroes and that heroes heroes that the the heroes that the that\n",
            "heroes heroes heroes that heroes that heroes the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l-VXCnStkvc"
      },
      "source": [
        "def generate_text(seed_text, next_words, _model, max_sequence_len, temp=0.2):\n",
        "    res = dict((v,k) for k,v in tokenizer.word_index.items())\n",
        "    a = []\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        _pred = _model.predict(token_list)\n",
        "        #pred = np.argmax(_pred, axis=1)[0]\n",
        "        pred = sample(_pred[0], temp)\n",
        "        a.append(max(_pred[0]))\n",
        "\n",
        "        seed_text += \" \" + res[pred]\n",
        "    return seed_text, a\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 64,
      "outputs": []
    }
  ]
}